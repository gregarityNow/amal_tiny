# -*- coding: utf-8 -*-
"""amal_proj_tiny.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dmt671i4m3zNsPdd9yxHNFrnaUK_Cv8f
"""

# !pip install transformers
# !pip install datasets

from transformers import AutoImageProcessor#, SwinForImageClassification

import torch
from datasets import load_dataset
dataset = load_dataset("huggingface/cats-image")





visualize_loss_acc(allLosses, allAccs, compRates)

with torch.no_grad():
    torch.cuda.empty_cache()
import gc
gc.collect()
!rm /content/adapter_*
allLosses, allAccs, compRates = train_ViT(quickie=100,batch_size=16,
                targetCompRate=8,n_epochs=5,ro=0.5,comp_n_epochs=5,hid_size=50)
visualize_loss_acc(allLosses, allAccs, compRates)

train_loader, test_loader = get_mnist_loaders()
for images, labels in train_loader:
    print(images[0].shape)
    plt.imshow(np.transpose(images[0],[1,2,0]))
    print(labels[0])
    break

import gc
!pip install pynvml
def report_gpu():
   print(torch.cuda.list_gpu_processes())
   gc.collect()
   torch.cuda.empty_cache()

report_gpu()

# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi
# !pip install gputil
# !pip install psutil
# !pip install humanize

# import psutil
# import humanize
# import os
# import GPUtil as GPU

GPUs = GPU.getGPUs()
# XXX: only one GPU on Colab and isnâ€™t guaranteed
gpu = GPUs[0]
def printm():
    process = psutil.Process(os.getpid())
    print("Gen RAM Free: " + humanize.naturalsize(psutil.virtual_memory().available), " |     Proc size: " + humanize.naturalsize(process.memory_info().rss))
    print("GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))
printm()

inputs = image_processor(image, return_tensors="pt")
inputs["pixel_values"].shape
vit_tina = ViT_TINA(5,n_classes=1000)
print(vit_tina)
with torch.no_grad():
    logits = vit_tina(inputs).logits
    # logits = modelViT(**inputs).logits

# model predicts one of the 1000 ImageNet classes
predicted_label = logits.argmax(-1).item()
modelViT.config.id2label[predicted_label]

"""TODO (basic):

*   Implement training loop, make sure to freeze non-Adapter blocks (by not passing them to the optimizer)
*   Implement neuron score function, pruning (according to TINA)
*   Find datasets to fine-tune on

TODO (analysis)

*   Compare with completely fine-tuned models
*   Come up with different ways to adapt? HP-tuning (layer size, depth?), how many adapters, etc.


"""